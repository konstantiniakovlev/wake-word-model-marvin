{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, LSTM, Dense, TimeDistributed, Dropout, BatchNormalization, Conv1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "\n",
    "from scripts.layers import Spectrogram, MelSpectrogram, LogMelSpectrogram\n",
    "from scripts.metrics import Precision, Recall, F1\n",
    "from scripts.data_generator import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get training data stats and parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_path = 'data/train/'\n",
    "nr_x_files = int(len(os.listdir(train_path)) / 2)\n",
    "x_filepaths = [train_path + f'X{i}.npy' for i in range(nr_x_files)]\n",
    "\n",
    "data_stats = {}\n",
    "means = []\n",
    "stds = []\n",
    "mins = []\n",
    "maxs = []\n",
    "\n",
    "for filepath in tqdm.tqdm(x_filepaths):\n",
    "    x = np.load(filepath)\n",
    "\n",
    "    means.append(x.mean())\n",
    "    stds.append(x.std())\n",
    "    mins.append(x.min())\n",
    "    maxs.append(x.max())\n",
    "\n",
    "data_stats['x_min'] = min(mins)\n",
    "data_stats['x_max'] = max(maxs)\n",
    "data_stats['data_mean'] = np.mean(means)\n",
    "data_stats['data_std'] = np.mean(stds)\n",
    "\n",
    "with open('stats/data_stats.json', 'w') as outfile:\n",
    "    json.dump(data_stats, outfile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load params and hparams\n",
    "with open('stats/params.json') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "with open('stats/hparams.json') as f:\n",
    "    hparams = json.load(f)\n",
    "\n",
    "# get additional data stats\n",
    "with open('stats/data_stats.json') as json_file:\n",
    "    data_stats = json.load(json_file)\n",
    "\n",
    "params.update(data_stats)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Generators"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# get file paths for each set\n",
    "train_path = 'data/train/'\n",
    "val_path = 'data/val/'\n",
    "test_path = 'data/test/'\n",
    "\n",
    "nr_train_files = int(len(os.listdir(train_path)) / 2)\n",
    "nr_val_files = int(len(os.listdir(val_path)) / 2)\n",
    "nr_test_files = int(len(os.listdir(test_path)) / 2)\n",
    "\n",
    "filenames_X_train = [train_path + f'X{i}.npy' for i in range(nr_train_files)]\n",
    "filenames_X_val = [val_path + f'X{i}.npy' for i in range(nr_val_files)]\n",
    "filenames_X_test = [test_path + f'X{i}.npy' for i in range(nr_test_files)]\n",
    "\n",
    "filenames_y_train = [train_path + f'y{i}.npy' for i in range(nr_train_files)]\n",
    "filenames_y_val = [val_path + f'y{i}.npy' for i in range(nr_val_files)]\n",
    "filenames_y_test = [test_path + f'y{i}.npy' for i in range(nr_test_files)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# data generators\n",
    "train_generator = DataGenerator(filenames_X_train, filenames_y_train)\n",
    "val_generator = DataGenerator(filenames_X_val, filenames_y_val)\n",
    "test_generator = DataGenerator(filenames_X_test, filenames_y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Callbacks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# create callback directory\n",
    "callback_dir = 'callbacks/'\n",
    "if not os.path.isdir(callback_dir):\n",
    "        os.mkdir(callback_dir)\n",
    "\n",
    "# create check pointer\n",
    "nr_batches = 336\n",
    "model_path = callback_dir + '/model-best.h5'\n",
    "checkpointer = ModelCheckpoint(\n",
    "    model_path,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=True,\n",
    "    save_freq=nr_batches * 10  # number of batches to save progress. - every 10 epochs = nr_batches * nr_epochs\n",
    ")\n",
    "\n",
    "# create early stopper\n",
    "earlystopper = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = hparams['lr']\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "\n",
    "    if epoch < 10:\n",
    "        return initial_lrate\n",
    "\n",
    "    elif epoch % 10 == 0:\n",
    "        lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "\n",
    "        if lrate < 1e-5:\n",
    "            lrate = 1e-5\n",
    "\n",
    "        print(f'\\nChanging learning rate to {lrate}\\n')\n",
    "\n",
    "        return lrate\n",
    "\n",
    "    else:\n",
    "        lrate = initial_lrate * math.pow(drop,\n",
    "                                         math.floor((1+epoch - (epoch % 10))/epochs_drop))\n",
    "        return lrate\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(step_decay)\n",
    "\n",
    "callbacks = [checkpointer, earlystopper, lr_scheduler]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = Adam(learning_rate=hparams['lr'])\n",
    "\n",
    "# metrics\n",
    "metrics = [\n",
    "    Precision,\n",
    "    Recall,\n",
    "    F1\n",
    "]\n",
    "\n",
    "# input\n",
    "model = Sequential(name='Trigger-word-Marvin-model')\n",
    "model.add(tf.keras.layers.Input(shape=(params['sample_rate'],),\n",
    "                                dtype=tf.float32))\n",
    "\n",
    "# preprocessing\n",
    "preprocessing = Sequential([\n",
    "    Spectrogram(params, hparams),\n",
    "    MelSpectrogram(params, hparams),\n",
    "    LogMelSpectrogram(params, hparams),\n",
    "], name='preprocessing_layer')\n",
    "model.add(preprocessing)\n",
    "\n",
    "# convolutional units\n",
    "conv = Sequential([\n",
    "    Conv1D(hparams['num_conv_filters'],\n",
    "           hparams['conv_kernel_size'],\n",
    "           hparams['conv_stride'])\n",
    "], name='conv_layer')\n",
    "model.add(conv)\n",
    "\n",
    "# recurrent units\n",
    "rnn = Sequential([\n",
    "    GRU(hparams['gru_units'], return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    GRU(hparams['gru_units'], return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    GRU(hparams['gru_units'], return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    GRU(hparams['gru_units'], return_sequences=False)\n",
    "], name='rnn_layer')\n",
    "model.add(rnn)\n",
    "\n",
    "# dense units\n",
    "dense = Sequential([\n",
    "    Dense(1, activation='sigmoid',\n",
    "          bias_initializer=initializers.Constant(-2.197))\n",
    "], name='dense_layer')\n",
    "model.add(dense)\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=hparams['loss_function'],\n",
    "    metrics=metrics\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=hparams['epochs'],\n",
    "    callbacks=callbacks\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluation\n",
    "model.evaluate(test_generator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "ax[0].set_title('Model loss')\n",
    "ax[0].plot(history.history['loss'], label='train')\n",
    "ax[0].plot(history.history['val_loss'], label='val')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('Precision')\n",
    "ax[1].plot(history.history['Precision'], label='train')\n",
    "ax[1].plot(history.history['val_Precision'], label='val')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_ylabel('precision')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].set_title('Recall')\n",
    "ax[2].plot(history.history['Recall'], label='train')\n",
    "ax[2].plot(history.history['val_Recall'], label='val')\n",
    "ax[2].set_xlabel('epoch')\n",
    "ax[2].set_ylabel('recall')\n",
    "ax[2].legend()\n",
    "\n",
    "ax[3].set_title('F1')\n",
    "ax[3].plot(history.history['F1'], label='train')\n",
    "ax[3].plot(history.history['val_F1'], label='val')\n",
    "ax[3].set_xlabel('epoch')\n",
    "ax[3].set_ylabel('f1')\n",
    "ax[3].legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save model\n",
    "model_dir = 'models/'\n",
    "\n",
    "# serialize model to json\n",
    "model_json = model.to_json()\n",
    "\n",
    "with open(model_dir + 'marvin-model-struct.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(model_dir + \"marvin-model-weights.h5\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}